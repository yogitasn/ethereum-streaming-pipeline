{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83818ad-ef77-4fe5-bced-8c0b7299a4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting web3\n  Downloading web3-7.13.0-py3-none-any.whl.metadata (5.6 kB)\nCollecting eth-abi>=5.0.1 (from web3)\n  Downloading eth_abi-5.2.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting eth-account>=0.13.6 (from web3)\n  Downloading eth_account-0.13.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting eth-hash>=0.5.1 (from eth-hash[pycryptodome]>=0.5.1->web3)\n  Downloading eth_hash-0.7.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting eth-typing>=5.0.0 (from web3)\n  Downloading eth_typing-5.2.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting eth-utils>=5.0.0 (from web3)\n  Downloading eth_utils-5.3.1-py3-none-any.whl.metadata (5.7 kB)\nCollecting hexbytes>=1.2.0 (from web3)\n  Downloading hexbytes-1.3.1-py3-none-any.whl.metadata (3.3 kB)\nCollecting aiohttp>=3.7.4.post0 (from web3)\n  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: pydantic>=2.4.0 in /databricks/python3/lib/python3.12/site-packages (from web3) (2.10.6)\nRequirement already satisfied: requests>=2.23.0 in /databricks/python3/lib/python3.12/site-packages (from web3) (2.32.3)\nRequirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from web3) (4.12.2)\nCollecting types-requests>=2.0.0 (from web3)\n  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\nCollecting websockets<16.0.0,>=10.0.0 (from web3)\n  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting pyunormalize>=15.0.0 (from web3)\n  Downloading pyunormalize-17.0.0-py3-none-any.whl.metadata (5.7 kB)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.7.4.post0->web3)\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.4.0 (from aiohttp>=3.7.4.post0->web3)\n  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.12/site-packages (from aiohttp>=3.7.4.post0->web3) (24.3.0)\nCollecting frozenlist>=1.1.1 (from aiohttp>=3.7.4.post0->web3)\n  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp>=3.7.4.post0->web3)\n  Downloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp>=3.7.4.post0->web3)\n  Downloading propcache-0.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp>=3.7.4.post0->web3)\n  Downloading yarl-1.21.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (74 kB)\nCollecting parsimonious<0.11.0,>=0.10.0 (from eth-abi>=5.0.1->web3)\n  Downloading parsimonious-0.10.0-py3-none-any.whl.metadata (25 kB)\nCollecting bitarray>=2.4.0 (from eth-account>=0.13.6->web3)\n  Downloading bitarray-3.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nCollecting eth-keyfile<0.9.0,>=0.7.0 (from eth-account>=0.13.6->web3)\n  Downloading eth_keyfile-0.8.1-py3-none-any.whl.metadata (8.5 kB)\nCollecting eth-keys>=0.4.0 (from eth-account>=0.13.6->web3)\n  Downloading eth_keys-0.7.0-py3-none-any.whl.metadata (13 kB)\nCollecting eth-rlp>=2.1.0 (from eth-account>=0.13.6->web3)\n  Downloading eth_rlp-2.2.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting rlp>=1.0.0 (from eth-account>=0.13.6->web3)\n  Downloading rlp-4.1.0-py3-none-any.whl.metadata (3.2 kB)\nCollecting ckzg>=2.0.0 (from eth-account>=0.13.6->web3)\n  Downloading ckzg-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (887 bytes)\nCollecting pycryptodome<4,>=3.6.6 (from eth-hash[pycryptodome]>=0.5.1->web3)\n  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting cytoolz>=0.10.1 (from eth-utils>=5.0.0->web3)\n  Downloading cytoolz-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2.4.0->web3) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic>=2.4.0->web3) (2.27.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.23.0->web3) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.23.0->web3) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.23.0->web3) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.23.0->web3) (2025.1.31)\nCollecting toolz>=0.8.0 (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3)\n  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\nCollecting regex>=2022.3.15 (from parsimonious<0.11.0,>=0.10.0->eth-abi>=5.0.1->web3)\n  Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nDownloading web3-7.13.0-py3-none-any.whl (1.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m52.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading eth_abi-5.2.0-py3-none-any.whl (28 kB)\nDownloading eth_account-0.13.7-py3-none-any.whl (587 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/587.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m587.5/587.5 kB\u001B[0m \u001B[31m24.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading eth_hash-0.7.1-py3-none-any.whl (8.0 kB)\nDownloading eth_typing-5.2.1-py3-none-any.whl (19 kB)\nDownloading eth_utils-5.3.1-py3-none-any.whl (102 kB)\nDownloading hexbytes-1.3.1-py3-none-any.whl (5.1 kB)\nDownloading pyunormalize-17.0.0-py3-none-any.whl (51 kB)\nDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\nDownloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\nDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\nDownloading bitarray-3.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (332 kB)\nDownloading ckzg-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\nDownloading cytoolz-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m38.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading eth_keyfile-0.8.1-py3-none-any.whl (7.5 kB)\nDownloading eth_keys-0.7.0-py3-none-any.whl (20 kB)\nDownloading eth_rlp-2.2.0-py3-none-any.whl (4.4 kB)\nDownloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\nDownloading multidict-6.6.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\nDownloading parsimonious-0.10.0-py3-none-any.whl (48 kB)\nDownloading propcache-0.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\nDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.3/2.3 MB\u001B[0m \u001B[31m49.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading rlp-4.1.0-py3-none-any.whl (19 kB)\nDownloading yarl-1.21.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\nDownloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/802.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m802.0/802.0 kB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading toolz-1.0.0-py3-none-any.whl (56 kB)\nInstalling collected packages: ckzg, bitarray, websockets, types-requests, toolz, regex, pyunormalize, pycryptodome, propcache, multidict, hexbytes, frozenlist, eth-typing, eth-hash, aiohappyeyeballs, yarl, parsimonious, cytoolz, aiosignal, eth-utils, aiohttp, rlp, eth-keys, eth-abi, eth-rlp, eth-keyfile, eth-account, web3\nSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 bitarray-3.7.1 ckzg-2.1.5 cytoolz-1.0.1 eth-abi-5.2.0 eth-account-0.13.7 eth-hash-0.7.1 eth-keyfile-0.8.1 eth-keys-0.7.0 eth-rlp-2.2.0 eth-typing-5.2.1 eth-utils-5.3.1 frozenlist-1.7.0 hexbytes-1.3.1 multidict-6.6.4 parsimonious-0.10.0 propcache-0.4.0 pycryptodome-3.23.0 pyunormalize-17.0.0 regex-2025.9.18 rlp-4.1.0 toolz-1.0.0 types-requests-2.32.4.20250913 web3-7.13.0 websockets-15.0.1 yarl-1.21.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install web3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "462569a1-8a65-41b2-a9ef-f774b0052c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Troubleshooting steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45832cd-b523-4d4d-827c-ce79d95bd6c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from web3 import Web3\n",
    "\n",
    "provider_uri = \"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\"\n",
    "w3 = Web3(Web3.HTTPProvider(provider_uri))\n",
    "\n",
    "try:\n",
    "    connected = w3.is_connected()\n",
    "    print(f\"Connected: {connected}\")\n",
    "    if connected:\n",
    "        latest = w3.eth.block_number\n",
    "        print(f\"Latest block: {latest}\")\n",
    "        print(\"Connection is working!\")\n",
    "    else:\n",
    "        print(\"Connection failed - not connected\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056aebef-ac00-4ba1-a288-b7023aa2751e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EthereumStream\").getOrCreate()\n",
    "\n",
    "# Test if registration works\n",
    "try:\n",
    "    spark.dataSource.register(EthereumDataSource)\n",
    "    print(\"✓ DataSource registered successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Registration failed: {e}\")\n",
    "\n",
    "# List registered data sources to verify\n",
    "print(\"\\nRegistered data sources:\")\n",
    "# This may not work on all Spark versions, but try it\n",
    "try:\n",
    "    print(spark.catalog.listTables())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Test instantiation directly\n",
    "try:\n",
    "    test_options = {\n",
    "        \"provider_uri\": \"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\",\n",
    "        \"start_block\": \"19000000\"\n",
    "    }\n",
    "    test_source = EthereumDataSource(test_options)\n",
    "    print(f\"✓ DataSource instantiated: {test_source.name()}\")\n",
    "    print(f\"✓ Schema: {test_source.schema()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Instantiation failed: {e}\")\n",
    "\n",
    "# Now try the stream\n",
    "checkpoint_path = \"/Volumes/blockchain/ethereum/checkpoints/\"\n",
    "output_path = \"/Volumes/blockchain/ethereum/output/\"\n",
    "\n",
    "print(f\"\\nCheckpoint: {checkpoint_path}\")\n",
    "print(f\"Output: {output_path}\")\n",
    "\n",
    "try:\n",
    "    df = spark.readStream \\\n",
    "        .format(\"ethereum\") \\\n",
    "        .option(\"provider_uri\", \"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\") \\\n",
    "        .option(\"start_block\", \"19000000\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"✓ Stream created\")\n",
    "    print(f\"Schema: {df.schema}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Stream creation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec7375a-a3c7-4f9e-90ed-ce6f4ea72b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EthereumStream\").getOrCreate()\n",
    "\n",
    "checkpoint_path = \"/Volumes/blockchain/ethereum/checkpoints/\"\n",
    "output_path = \"/Volumes/blockchain/ethereum/output/\"\n",
    "\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Output: {output_path}\")\n",
    "\n",
    "spark.dataSource.register(EthereumDataSource)\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"ethereum\") \\\n",
    "    .option(\"provider_uri\", \"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\") \\\n",
    "    .option(\"start_block\", \"19000000\") \\\n",
    "    .option(\"max_calls_per_second\", \"0.5\") \\\n",
    "    .option(\"batch_size\", \"5\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"✓ Stream DataFrame created\")\n",
    "\n",
    "# Now try to start the write\n",
    "try:\n",
    "    query = df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .trigger(processingTime=\"30 seconds\") \\\n",
    "        .start(output_path)\n",
    "    \n",
    "    print(\"✓ Write stream started!\")\n",
    "    print(f\"Query ID: {query.id}\")\n",
    "    print(f\"Query name: {query.name}\")\n",
    "    \n",
    "    # Wait a bit and check status\n",
    "    time.sleep(10)\n",
    "    \n",
    "    print(f\"\\nQuery is active: {query.isActive}\")\n",
    "    print(f\"Query status: {query.status}\")\n",
    "    \n",
    "    if query.exception():\n",
    "        print(f\"\\n✗ Query has exception: {query.exception()}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No exceptions yet\")\n",
    "    \n",
    "    # Check progress\n",
    "    progress = query.recentProgress\n",
    "    if progress:\n",
    "        print(f\"\\nRecent progress: {len(progress)} batches\")\n",
    "        for i, p in enumerate(progress[-3:]):  # Last 3 batches\n",
    "            print(f\"  Batch {i}: {p}\")\n",
    "    \n",
    "    # Don't block - let it run in background\n",
    "    # query.awaitTermination()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Failed to start write stream: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538cb750-280b-4923-988e-7c225e4cd4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check what latestOffset is returning\n",
    "\n",
    "print(\"\\nManually testing the streaming reader:\")\n",
    "test_reader = EthereumStreamReader(\n",
    "    schema=StructType([\n",
    "        StructField(\"block_number\", LongType()),\n",
    "        StructField(\"block_hash\", StringType()),\n",
    "        StructField(\"timestamp\", LongType()),\n",
    "        StructField(\"tx_count\", LongType())\n",
    "    ]),\n",
    "    options={\n",
    "        \"provider_uri\": \"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\",\n",
    "        \"start_block\": \"19000000\",\n",
    "        \"batch_size\": \"5\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nTesting initialOffset:\")\n",
    "initial = test_reader.initialOffset()\n",
    "print(f\"Initial offset: {initial}\")\n",
    "\n",
    "print(\"\\nTesting latestOffset:\")\n",
    "try:\n",
    "    latest = test_reader.latestOffset()\n",
    "    print(f\"Latest offset: {latest}\")\n",
    "except Exception as e:\n",
    "    print(f\"latestOffset FAILED: {e}\")\n",
    "\n",
    "print(\"\\nTesting partitions:\")\n",
    "try:\n",
    "    parts = test_reader.partitions(initial, latest)\n",
    "    print(f\"Number of partitions created: {len(parts)}\")\n",
    "    if parts:\n",
    "        print(f\"First partition: blocks {parts[0].start} to {parts[0].end}\")\n",
    "except Exception as e:\n",
    "    print(f\"partitions FAILED: {e}\")\n",
    "\n",
    "print(\"\\nTesting read on first partition:\")\n",
    "try:\n",
    "    if parts:\n",
    "        rows = list(test_reader.read(parts[0]))\n",
    "        print(f\"Read {len(rows)} rows\")\n",
    "        if rows:\n",
    "            print(f\"First row: {rows[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"read FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ed8ea9-5f49-4046-b07a-cd5bfdd3ebf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Main code starts**...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7222ec90-12ba-4025-bb0b-dc9696adbb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, json, time, logging\n",
    "from pyspark.sql.datasource import DataSource, DataSourceStreamReader, InputPartition\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "from pyspark.sql import Row\n",
    "from web3 import Web3\n",
    "\n",
    "            \n",
    "\n",
    "# -----------------------------\n",
    "# Logging setup\n",
    "# -----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s [%(levelname)s] [%(name)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(\"EthereumStream\")\n",
    "\n",
    "# -----------------------------\n",
    "# Partition class\n",
    "# -----------------------------\n",
    "class BlockRangePartition(InputPartition):\n",
    "    def __init__(self, start, end):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        logger.debug(f\"Created partition: blocks {start} to {end}\")\n",
    "\n",
    "# -----------------------------\n",
    "# StreamReader\n",
    "# -----------------------------\n",
    "class EthereumStreamReader(DataSourceStreamReader):\n",
    "    def __init__(self, schema, options):\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Initializing EthereumStreamReader\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        self.schema = schema\n",
    "        self.options = options\n",
    "        self.provider_uri = options.get(\"provider_uri\")\n",
    "        self.start_block = int(options.get(\"start_block\", 1))\n",
    "        self.output_dir = options.get(\"output_dir\", \"dbfs:/Volumes/blockchain/ethereum/blocks/raw\")\n",
    "        self.current_block = self.start_block\n",
    "        self.max_calls_per_second = float(options.get(\"max_calls_per_second\", 0.8))\n",
    "\n",
    "        logger.info(f\"Configuration:\")\n",
    "        logger.info(f\"  - Provider URI: {self.provider_uri}\")\n",
    "        logger.info(f\"  - Start block: {self.start_block}\")\n",
    "        logger.info(f\"  - Output directory: {self.output_dir}\")\n",
    "        logger.info(f\"  - Max calls/sec: {self.max_calls_per_second}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "\n",
    "    # -------------------------\n",
    "    # Offset handling (dict-based)\n",
    "    # -------------------------\n",
    "    def initialOffset(self):\n",
    "        offset = {\"offset\": self.current_block}\n",
    "        logger.info(f\"initialOffset() called -> Returning: {offset}\")\n",
    "        return offset\n",
    "\n",
    "    def latestOffset(self):\n",
    "        logger.info(\"latestOffset() called -> Connecting to Ethereum provider...\")\n",
    "        try:\n",
    "            w3 = Web3(Web3.HTTPProvider(self.provider_uri))\n",
    "            if not w3.is_connected():\n",
    "                logger.error(\"Failed to connect to Ethereum provider in latestOffset()\")\n",
    "                raise RuntimeError(\"Failed to connect to Ethereum provider\")\n",
    "            \n",
    "            latest = w3.eth.block_number\n",
    "            offset = {\"offset\": latest}\n",
    "            logger.info(f\"latestOffset() -> Latest block from chain: {latest}\")\n",
    "            logger.info(f\"latestOffset() -> Returning: {offset}\")\n",
    "            return offset\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in latestOffset(): {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    # -------------------------\n",
    "    # Partition planning\n",
    "    # -------------------------\n",
    "    def partitions(self, start_offset, end_offset):\n",
    "        \"\"\"Legacy method name - calls planPartitions internally\"\"\"\n",
    "        return self.planPartitions(start_offset, end_offset)\n",
    "    \n",
    "    def planPartitions(self, start_offset, end_offset):\n",
    "        logger.info(\"-\" * 60)\n",
    "        logger.info(\"planPartitions() called\")\n",
    "        logger.info(f\"  - start_offset: {start_offset}\")\n",
    "        logger.info(f\"  - end_offset: {end_offset}\")\n",
    "        \n",
    "        start = start_offset.get(\"offset\", self.start_block) if start_offset else self.start_block\n",
    "        end = end_offset.get(\"offset\", start + 100) if end_offset else start + 100\n",
    "        step = int(self.options.get(\"batch_size\", 10))\n",
    "        \n",
    "        logger.info(f\"  - Computed start: {start}\")\n",
    "        logger.info(f\"  - Computed end: {end}\")\n",
    "        logger.info(f\"  - Batch size: {step}\")\n",
    "        \n",
    "        partitions = []\n",
    "        for i in range(start, end + 1, step):\n",
    "            partition_end = min(i + step - 1, end)\n",
    "            partitions.append(BlockRangePartition(i, partition_end))\n",
    "        \n",
    "        logger.info(f\"  - Created {len(partitions)} partition(s)\")\n",
    "        for idx, p in enumerate(partitions):\n",
    "            logger.info(f\"    Partition {idx}: blocks {p.start} to {p.end} ({p.end - p.start + 1} blocks)\")\n",
    "        logger.info(\"-\" * 60)\n",
    "        \n",
    "        return partitions\n",
    "\n",
    "    # -------------------------\n",
    "    # Reader\n",
    "    # -------------------------\n",
    "    def read(self, partition):\n",
    "        logger.info(\"*\" * 60)\n",
    "        logger.info(f\"read() called for partition: blocks {partition.start} to {partition.end}\")\n",
    "        logger.info(\"*\" * 60)\n",
    "        \n",
    "        try:\n",
    "            w3 = Web3(Web3.HTTPProvider(self.provider_uri))\n",
    "            if not w3.is_connected():\n",
    "                logger.error(\"Failed to connect to Ethereum provider in read()\")\n",
    "                raise RuntimeError(\"Failed to connect to Ethereum provider\")\n",
    "            \n",
    "            logger.info(\"Successfully connected to Ethereum provider\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection error in read(): {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        last_call_time = 0\n",
    "        blocks_processed = 0\n",
    "        blocks_failed = 0\n",
    "        \n",
    "        for blk_num in range(partition.start, partition.end + 1):\n",
    "            try:\n",
    "                # Rate limiting\n",
    "                now = time.time()\n",
    "                sleep_time = (1 / self.max_calls_per_second) - (now - last_call_time)\n",
    "                if sleep_time > 0:\n",
    "                    logger.debug(f\"Rate limiting: sleeping for {sleep_time:.3f}s\")\n",
    "                    time.sleep(sleep_time)\n",
    "                last_call_time = time.time()\n",
    "\n",
    "                logger.info(f\"Fetching block {blk_num}...\")\n",
    "                block = w3.eth.get_block(blk_num, full_transactions=False)\n",
    "\n",
    "                blk_dict = {\n",
    "                    \"block_number\": block.number,\n",
    "                    \"block_hash\": block.hash.hex(),\n",
    "                    \"timestamp\": block.timestamp,\n",
    "                    \"tx_count\": len(block.transactions)\n",
    "                }\n",
    "\n",
    "                # Save JSON locally (optional)\n",
    "                file_path = os.path.join(self.output_dir, f\"block_{blk_num}.json\")\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    json.dump(blk_dict, f, indent=2)\n",
    "                logger.debug(f\"Saved block data to {file_path}\")\n",
    "\n",
    "                logger.info(f\"✓ Block {blk_num}: hash={block.hash.hex()[:10]}..., \"\n",
    "                           f\"timestamp={block.timestamp}, tx_count={len(block.transactions)}\")\n",
    "\n",
    "                yield Row(\n",
    "                    block_number=block.number,\n",
    "                    block_hash=block.hash.hex(),\n",
    "                    timestamp=block.timestamp,\n",
    "                    tx_count=len(block.transactions)\n",
    "                )\n",
    "                \n",
    "                self.current_block = blk_num + 1\n",
    "                blocks_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                blocks_failed += 1\n",
    "                logger.error(f\"✗ Error fetching block {blk_num}: {e}\", exc_info=True)\n",
    "                continue\n",
    "        \n",
    "        logger.info(\"*\" * 60)\n",
    "        logger.info(f\"read() completed for partition {partition.start}-{partition.end}\")\n",
    "        logger.info(f\"  - Blocks processed: {blocks_processed}\")\n",
    "        logger.info(f\"  - Blocks failed: {blocks_failed}\")\n",
    "        logger.info(\"*\" * 60)\n",
    "\n",
    "    # -------------------------\n",
    "    # Commit method for checkpointing\n",
    "    # -------------------------\n",
    "    def commit(self, end_offset):\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"commit() called with end_offset: {end_offset}\")\n",
    "        logger.info(f\"Successfully committed up to block: {end_offset.get('offset')}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "\n",
    "    # -------------------------\n",
    "    # Pickle support\n",
    "    # -------------------------\n",
    "    def __getstate__(self):\n",
    "        logger.debug(\"__getstate__() called for serialization\")\n",
    "        state = self.__dict__.copy()\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        logger.debug(\"__setstate__() called for deserialization\")\n",
    "        self.__dict__.update(state)\n",
    "\n",
    "# -----------------------------\n",
    "# DataSource wrapper\n",
    "# -----------------------------\n",
    "class EthereumDataSource(DataSource):\n",
    "    def __init__(self, options):\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"EthereumDataSource.__init__() called\")\n",
    "        logger.info(f\"Options: {options}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        self.options = options\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        logger.debug(\"name() called -> returning 'ethereum'\")\n",
    "        return \"ethereum\"\n",
    "\n",
    "    def schema(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"block_number\", LongType()),\n",
    "            StructField(\"block_hash\", StringType()),\n",
    "            StructField(\"timestamp\", LongType()),\n",
    "            StructField(\"tx_count\", LongType())\n",
    "        ])\n",
    "        logger.info(\"schema() called -> returning schema with 4 fields\")\n",
    "        logger.debug(f\"Schema: {schema}\")\n",
    "        return schema\n",
    "\n",
    "    def streamReader(self, schema):\n",
    "        logger.info(\"streamReader() called -> creating EthereumStreamReader\")\n",
    "        return EthereumStreamReader(schema, self.options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bec2d5-f1e0-4dae-b29e-89cd91c45993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using checkpoint: /Volumes/blockchain/ethereum/checkpoints/\n✅ Using output: /Volumes/blockchain/ethereum/output/\nLatest Ethereum block number: 23516091\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 03:37:55 [INFO] [EthereumStream] Streaming query started. Awaiting termination...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import uuid\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "from web3 import Web3\n",
    "\n",
    "# --- Spark session ---\n",
    "spark = SparkSession.builder.appName(\"EthereumStream\").getOrCreate()\n",
    "\n",
    "# --- Dynamic checkpoint/output paths ---\n",
    "base_checkpoint = \"/Volumes/blockchain/ethereum/checkpoints\"\n",
    "base_output = \"/Volumes/blockchain/ethereum/output\"\n",
    "\n",
    "\n",
    "eth_schema = StructType([\n",
    "    StructField(\"block_number\", LongType(), True),\n",
    "    StructField(\"block_hash\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"tx_count\", LongType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "checkpoint_path = f\"{base_checkpoint}/\"\n",
    "output_path = f\"{base_output}/\"\n",
    "\n",
    "print(\"✅ Using checkpoint:\", checkpoint_path)\n",
    "print(\"✅ Using output:\", output_path)\n",
    "\n",
    "spark.dataSource.register(EthereumDataSource)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, ArrayType, MapType\n",
    "\n",
    "\n",
    "# Connect to an Ethereum node (Infura, Alchemy, etc.)\n",
    "w3 = Web3(Web3.HTTPProvider(\"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\"))\n",
    "\n",
    "# Get latest block number\n",
    "start_block = w3.eth.block_number\n",
    "print(\"Latest Ethereum block number:\", start_block)\n",
    "\n",
    "\n",
    "\n",
    "# --- Read from custom Ethereum source ---\n",
    "df = spark.readStream \\\n",
    "    .format(\"ethereum\") \\\n",
    "    .schema(eth_schema) \\\n",
    "    .option(\"provider_uri\", \"https://mainnet.infura.io/v3/6504e6a7883c4b49ac1cf17099e2ea3a\") \\\n",
    "    .option(\"start_block\", start_block) \\\n",
    "    .option(\"poll_interval\", 15) \\\n",
    "    .load()\n",
    "\n",
    "# --- Write with dynamic paths ---\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(output_path)\n",
    "\n",
    "#availableNow\n",
    "logger.info(\"Streaming query started. Awaiting termination...\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5021963599579255,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ethereum-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}