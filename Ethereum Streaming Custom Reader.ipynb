{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1019a572-0c9c-4f8d-b8ec-488c6e388caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "\n",
    "# Block schema\n",
    "block_schema = StructType([\n",
    "    StructField(\"block_number\", LongType()),\n",
    "    StructField(\"block_hash\", StringType()),\n",
    "    StructField(\"miner\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Transaction schema\n",
    "tx_schema = StructType([\n",
    "    StructField(\"tx_hash\", StringType()),\n",
    "    StructField(\"block_number\", LongType()),\n",
    "    StructField(\"from_address\", StringType()),\n",
    "    StructField(\"to_address\", StringType()),\n",
    "    StructField(\"value\", DoubleType()),\n",
    "    StructField(\"timestamp\", TimestampType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f10e619a-49e7-4f57-9ac1-b85ec6921705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from web3.datastructures import AttributeDict\n",
    "from hexbytes import HexBytes\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "import json\n",
    "\n",
    "def to_serializable(obj):\n",
    "    if isinstance(obj, HexBytes):\n",
    "        return obj.hex()\n",
    "    elif isinstance(obj, AttributeDict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_serializable(i) for i in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def block_to_row(block):\n",
    "    b = to_serializable(block)\n",
    "    return Row(\n",
    "        block_number=b[\"number\"],\n",
    "        block_hash=b[\"hash\"],\n",
    "        miner=b[\"miner\"],\n",
    "        timestamp=datetime.fromtimestamp(b[\"timestamp\"])\n",
    "    )\n",
    "\n",
    "def tx_to_row(tx, block_number, timestamp):\n",
    "    tx_dict = to_serializable(tx)\n",
    "    return Row(\n",
    "        tx_hash=tx_dict[\"hash\"],\n",
    "        block_number=block_number,\n",
    "        from_address=tx_dict[\"from\"],\n",
    "        to_address=tx_dict[\"to\"],\n",
    "        value=float(tx_dict[\"value\"])/1e18,  # convert wei to ether\n",
    "        timestamp=timestamp\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42eb4706-b203-456a-a60f-fd3f0e46f2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.datasource import DataSource, DataSourceStreamReader, DataSourceStreamWriter\n",
    "from web3 import Web3\n",
    "import os\n",
    "import time\n",
    "\n",
    "provider_uri = \"https://mainnet.infura.io/v3/YOUR_INFURA_CODE\"\n",
    "w3 = Web3(Web3.HTTPProvider(provider_uri))\n",
    "\n",
    "base_volume = \"/Volumes/ethereum_catalog/ethereum/ethereum-volume\"\n",
    "\n",
    "# Volume paths\n",
    "raw_blocks_dir = f\"{base_volume}/raw/blocks/\"\n",
    "raw_tx_dir = f\"{base_volume}/raw/transactions/\"\n",
    "#os.makedirs(raw_blocks_dir, exist_ok=True)\n",
    "#os.makedirs(raw_tx_dir, exist_ok=True)\n",
    "\n",
    "delta_blocks_path = f\"{base_volume}/delta/blocks/\"\n",
    "delta_tx_path = f\"{base_volume}/delta/transactions/\"\n",
    "checkpoint_blocks = f\"{base_volume}/checkpoints/blocks/\"\n",
    "checkpoint_tx = f\"{base_volume}/checkpoints/transactions/\"\n",
    "\n",
    "class EthereumStreamReader(DataSourceStreamReader):\n",
    "    def __init__(self, schema, options, entity=\"blocks\"):\n",
    "        self._schema = schema\n",
    "        self.options = options\n",
    "        self.entity = entity\n",
    "        self.last_block = w3.eth.block_number\n",
    "\n",
    "    def readSchema(self):\n",
    "        return self._schema\n",
    "\n",
    "    def planInputPartitions(self):\n",
    "        return []\n",
    "\n",
    "    def getRows(self):\n",
    "        latest_block = w3.eth.block_number\n",
    "        rows = []\n",
    "        for block_number in range(self.last_block + 1, latest_block + 1):\n",
    "            block = w3.eth.get_block(block_number, full_transactions=True)\n",
    "            timestamp = datetime.fromtimestamp(block.timestamp)\n",
    "\n",
    "            # Save raw JSON\n",
    "            if self.entity == \"blocks\":\n",
    "                file_path = os.path.join(raw_blocks_dir, f\"block_{block_number}.json\")\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    json.dump(to_serializable(block), f)\n",
    "                rows.append(block_to_row(block))\n",
    "            else:\n",
    "                for tx in block.transactions:\n",
    "                    file_path = os.path.join(raw_tx_dir, f\"tx_{tx.hash.hex()}.json\")\n",
    "                    with open(file_path, \"w\") as f:\n",
    "                        json.dump(to_serializable(tx), f)\n",
    "                    rows.append(tx_to_row(tx, block_number, timestamp))\n",
    "\n",
    "            print(f\"âœ… Processed block {block_number}\")\n",
    "        self.last_block = latest_block\n",
    "        time.sleep(10)  # poll interval\n",
    "        return rows\n",
    "\n",
    "class EthereumStreamWriter(DataSourceStreamWriter):\n",
    "    def __init__(self, options):\n",
    "        self.options = options\n",
    "\n",
    "    def createWriter(self, partitionId, taskId, epochId):\n",
    "        return None\n",
    "\n",
    "class EthereumCustomDataSource(DataSource):\n",
    "    @classmethod\n",
    "    def name(cls):\n",
    "        return \"ethereum_custom\"\n",
    "\n",
    "    def schema(self):\n",
    "        return block_schema\n",
    "\n",
    "    def streamReader(self, schema):\n",
    "        entity = self.options.get(\"entity\", \"blocks\")\n",
    "        s = block_schema if entity == \"blocks\" else tx_schema\n",
    "        return EthereumStreamReader(s, self.options, entity)\n",
    "\n",
    "    def streamWriter(self, schema, overwrite):\n",
    "        return EthereumStreamWriter(self.options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d256aa5-1c68-4fe7-b6ad-7e87a32c71f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Blocks stream\n",
    "df_blocks = spark.readStream.format(\"ethereum_custom\") \\\n",
    "    .option(\"entity\", \"blocks\") \\\n",
    "    .load(schema=block_schema)\n",
    "\n",
    "\n",
    "\n",
    "df_blocks.writeStream.format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_blocks) \\\n",
    "    .option(\"path\", delta_blocks_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f181b7e7-bf0d-4ccd-9ee2-b7680422463e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transactions stream\n",
    "df_tx = spark.readStream.format(\"ethereum_custom\") \\\n",
    "    .option(\"entity\", \"transactions\") \\\n",
    "    .load(schema=tx_schema)\n",
    "\n",
    "df_tx.writeStream.format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_tx) \\\n",
    "    .option(\"path\", delta_tx_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff94072-52b8-46a4-bc1b-e2ac60efc8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ethereum_blocks\n",
    "USING DELTA\n",
    "LOCATION '{delta_blocks_path}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ethereum_transactions\n",
    "USING DELTA\n",
    "LOCATION '{delta_tx_path}'\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ethereum Streaming Custom Reader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}